{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "num_epochs=100\n",
    "batch_size=64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-e27b1633b2e2>:1: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data.\n",
      "WARNING:tensorflow:From /home/aniket/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/aniket/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/aniket/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/aniket/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/aniket/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/aniket/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "train_data = mnist.train.images # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32).reshape((-1,))\n",
    "eval_data = mnist.test.images # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32).reshape((-1,))\n",
    "train=np.concatenate((train_data,train_labels),axis=1)\n",
    "eval=np.concatenate((eval_data,eval_labels),axis=1)\n",
    "np.random.shuffle(train)\n",
    "np.random.shuffle(eval)\n",
    "train_data,train_labels=train[:,:-1],train[:,-1]\n",
    "eval_data,eval_labels=eval[:,:-1],eval[:,-1]\n",
    "train_data_batches=np.array_split(train_data,math.ceil(train.shape[0]/batch_size),axis=0)\n",
    "train_label_batches=np.array_split(train_labels,math.ceil(train.shape[0]/batch_size),axis=0)\n",
    "eval_data_batches=np.array_split(eval_data,math.ceil(eval.shape[0]/batch_size),axis=0)\n",
    "eval_label_batches=np.array_split(eval_labels,math.ceil(eval.shape[0]/batch_size),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(24121999)\n",
    "class model():\n",
    "    def __init__(self,train_data_batches,train_label_batches,eval_data_batches,eval_label_batches):\n",
    "        with tf.device('/device:GPU:0'):\n",
    "            self.idx=0\n",
    "            \n",
    "            self.train_data_batches=[tf.constant(data) for data in train_data_batches]\n",
    "            self.eval_data_batches=[tf.constant(data) for data in eval_data_batches]\n",
    "            self.train_data_labels=[tf.constant(data) for data in train_label_batches]\n",
    "            self.eval_data_labels=[tf.constant(data) for data in eval_label_batches]\n",
    "            \n",
    "            self.x=self.train_data_batches[idx]\n",
    "            \n",
    "            self.x_t=tf.transpose(self.x)#f.random.normal([784,64])#tf.placeholder(tf.float32,shape=[784,None])\n",
    "            self.labels=self.train_data_labels[idx]\n",
    "            \n",
    "            self.sparsed_weights1=tf.contrib.layers.dense_to_sparse(tf.zeros([784,1000]))\n",
    "            self.sparsed_weights2=tf.contrib.layers.dense_to_sparse(tf.zeros([1000,1000]))\n",
    "            self.sparsed_weights3=tf.contrib.layers.dense_to_sparse(tf.zeros([1000,500]))\n",
    "            self.sparsed_weights4=tf.contrib.layers.dense_to_sparse(tf.zeros([500,200]))\n",
    "            self.last_layer=tf.zeros([10,200])\n",
    "            \n",
    "            \n",
    "            a1=tf.nn.relu(tf.sparse.matmul(self.sparsed_weights1,self.x_t,adjoint_a=True))\n",
    "            a2=tf.nn.relu(tf.sparse.matmul(self.sparsed_weights2,a1,adjoint_a=True))\n",
    "            a3=tf.nn.relu(tf.sparse.matmul(self.sparsed_weights3,a2,adjoint_a=True))\n",
    "            a4=tf.nn.relu(tf.sparse.matmul(self.sparsed_weights4,a3,adjoint_a=True))\n",
    "            self.sparse_preds=tf.matmul(self.last_layer,a4)\n",
    "\n",
    "\n",
    "            self.w_layer1= tf.get_variable(\"weights1\", [784,1000],initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "            self.w_layer2=tf.get_variable(\"weights2\", [1000,1000],initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "            self.w_layer3=tf.get_variable(\"weights3\", [1000,500],initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "            self.w_layer4=tf.get_variable(\"weights4\", [500,200],initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "            self.w_layer5=tf.get_variable(\"weights5\", [200,10],initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "            self.layer1=tf.nn.relu(tf.matmul(self.x,self.w_layer1))\n",
    "\n",
    "            self.layer2=tf.nn.relu(tf.matmul(self.layer1,self.w_layer2))\n",
    "            self.layer3=tf.nn.relu(tf.matmul(self.layer2,self.w_layer3))\n",
    "            self.layer4=tf.nn.relu(tf.matmul(self.layer3,self.w_layer4))\n",
    "            self.logits=tf.matmul(self.layer4,self.w_layer5)\n",
    "            self.probs=tf.nn.softmax(self.logits,axis=1)\n",
    "            self.loss=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.labels,logits=self.logits)\n",
    "            self.loss=tf.reduce_mean(self.loss)\n",
    "            self.var=tf.trainable_variables()\n",
    "            self.var1=self.var[:-1]\n",
    "            self.opt=tf.train.AdamOptimizer(1e-5).minimize(self.loss)\n",
    "            self.percentile=tf.placeholder(tf.float32)\n",
    "            self.sparseunits=[self.weightsparsify(var,self.percentile) for i,var in enumerate(self.var1)]\n",
    "\n",
    "            self.saver=tf.train.Saver()     \n",
    "    \n",
    "        \n",
    "    def weightsparsify(self,wt,percentile):\n",
    "            #print(wt)\n",
    "            #print(percentile)\n",
    "            with tf.device('/device:GPU:0'):\n",
    "                percentile=tf.contrib.distributions.percentile(tf.abs(wt),percentile)\n",
    "\n",
    "                condition=tf.less(percentile,tf.abs(wt))\n",
    "                indices=tf.where(condition)\n",
    "                values=tf.gather_nd(wt,indices)\n",
    "                dense_shape=tf.shape(wt,out_type=tf.int64)\n",
    "                sparsetensor=tf.sparse.SparseTensor(indices,values,dense_shape)\n",
    "                densewieght=tf.sparse.to_dense(sparsetensor)\n",
    "                return sparsetensor\n",
    "    def create_dataset(self,batch_size,train_data,train_labels,eval_data,eval_labels): \n",
    "        with tf.device('/device:GPU:0'):\n",
    "            def gentrain():\n",
    "                for i in range(train_data.shape[0]):\n",
    "                    yield train_data[i,:], train_labels[i]\n",
    "            def gentest():\n",
    "                for i in range(eval_data.shape[0]):\n",
    "                    yield eval_data[i,:], eval_labels[i]\n",
    "\n",
    "            ds_train = tf.data.Dataset.from_generator(gentrain, (tf.float32, tf.int32))\n",
    "            ds_test=tf.data.Dataset.from_generator(gentest,(tf.float32, tf.int32))\n",
    "            return [ds_train.batch(batch_size),len(train_labels)],[ds_test.batch(batch_size),len(eval_labels)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aniket/.local/lib/python3.5/site-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n"
     ]
    }
   ],
   "source": [
    "model=model(batch_size,train_data,train_labels,eval_data,eval_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkequal(p,a):\n",
    "\tp=np.argmax(p,axis=1)\n",
    "\treturn np.sum(p==a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Trying to access resource _0_IteratorV2 located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n\t [[node make_initializer (defined at <ipython-input-6-6e7626c23d3c>:11)  = MakeIterator[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](BatchDatasetV2, IteratorV2/_7)]]\n\nCaused by op 'make_initializer', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/aniket/.local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/aniket/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/aniket/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-6e7626c23d3c>\", line 11, in <module>\n    sess.run(model.iterator.make_initializer(model.train_ds))\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 359, in make_initializer\n    dataset._as_variant_tensor(), self._iterator_resource, name=name)  # pylint: disable=protected-access\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2544, in make_iterator\n    \"MakeIterator\", dataset=dataset, iterator=iterator, name=name)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Trying to access resource _0_IteratorV2 located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n\t [[node make_initializer (defined at <ipython-input-6-6e7626c23d3c>:11)  = MakeIterator[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](BatchDatasetV2, IteratorV2/_7)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Trying to access resource _0_IteratorV2 located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n\t [[{{node make_initializer}} = MakeIterator[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](BatchDatasetV2, IteratorV2/_7)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6e7626c23d3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Trying to access resource _0_IteratorV2 located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n\t [[node make_initializer (defined at <ipython-input-6-6e7626c23d3c>:11)  = MakeIterator[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](BatchDatasetV2, IteratorV2/_7)]]\n\nCaused by op 'make_initializer', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/aniket/.local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/aniket/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/aniket/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-6e7626c23d3c>\", line 11, in <module>\n    sess.run(model.iterator.make_initializer(model.train_ds))\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 359, in make_initializer\n    dataset._as_variant_tensor(), self._iterator_resource, name=name)  # pylint: disable=protected-access\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2544, in make_iterator\n    \"MakeIterator\", dataset=dataset, iterator=iterator, name=name)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/home/aniket/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Trying to access resource _0_IteratorV2 located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n\t [[node make_initializer (defined at <ipython-input-6-6e7626c23d3c>:11)  = MakeIterator[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](BatchDatasetV2, IteratorV2/_7)]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "       \n",
    "            \n",
    "        sess.run(model.iterator.make_initializer(model.train_ds))\n",
    "        bar = tqdm(total=model.train_len)\n",
    "        ls=0\n",
    "        train_correct=0\n",
    "        train_total=0\n",
    "        cntbatches=0\n",
    "        while True:\n",
    "            try:\n",
    "                cntbatches+=1\n",
    "                l,_,p,y=sess.run([model.loss,model.opt,model.probs,model.labels])\n",
    "                #print(pr.numpy())\n",
    "                batch_size=p.shape[0]\n",
    "                bar.update(batch_size)\n",
    "                train_correct+=checkequal(p,y)\n",
    "                train_total+=y.shape[0]\n",
    "                ls+=l\n",
    "            except(tf.errors.OutOfRangeError, StopIteration):\n",
    "                bar.close()\n",
    "                break\n",
    "                \n",
    "        ls=ls/cntbatches\n",
    "        train_acc=float(train_correct)/float(train_total)\n",
    "       \n",
    "        sess.run(model.iterator.make_initializer(model.eval_ds_ready))\n",
    "        #tqdm.write('Training on epoch '+str(i))\n",
    "        bar = tqdm(total=model.eval_len)\n",
    "        test_correct=0\n",
    "        test_total=0\n",
    "        t2 = time.clock()\n",
    "        cnttestbatches=0\n",
    "        while True:\n",
    "            try:\n",
    "                cnttestbatches+=1\n",
    "                p,y=sess.run([model.probs,model.labels])\n",
    "                batch_size=p.shape[0]\n",
    "                bar.update(batch_size)\n",
    "                test_correct+=checkequal(p,y)\n",
    "                test_total+=y.shape[0]\n",
    "            except(tf.errors.OutOfRangeError, StopIteration):\n",
    "                bar.close()\n",
    "                break\n",
    "        \n",
    "        test_acc=float(test_correct)/float(test_total)\n",
    "        #print(test_acc)\n",
    "        t2 = time.clock()-t2\n",
    "        #print(t2)\n",
    "        print(str(i)+' loss:'+str(ls)+' train_acc:'+str(train_acc)+' test_acc:'+str(test_acc))\n",
    "        model.saver.save(sess,'model/notpruned/model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/notpruned/model.pb\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    model.saver.restore(sess,'model/notpruned/model.pb')\n",
    "    percentile=tf.placeholder(tf.float32)\n",
    "    \n",
    "    p_val=90\n",
    "    #p_val=p_val.astype(np.float32)\n",
    "    sp=sess.run(model.sparseunits,feed_dict={model.percentile:p_val})\n",
    "    \n",
    "        \n",
    "    last_layer=sess.run(model.w_layer5)\n",
    "    sp.append(last_layer)\n",
    "    #model.new_weights.append(last_layer)\n",
    "    #print(model.sparsified_weights)\n",
    "    #print(model.sparsified_weights)\n",
    "    #s=np.load('weights.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7969920000000101\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement = True,log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        test_correct=0\n",
    "        test_total=0\n",
    "        sparse_placeholder=sp\n",
    "        model.sparsed_weights=[]\n",
    "        with tf.device('/device:GPU:0'):\n",
    "            model.sparsed_weights1=tf.sparse.SparseTensor(sparse_placeholder[0][0],sparse_placeholder[0][1],sparse_placeholder[0][2])\n",
    "            model.sparsed_weights2=tf.sparse.SparseTensor(sparse_placeholder[1][0],sparse_placeholder[1][1],sparse_placeholder[1][2])\n",
    "            model.sparsed_weights3=tf.sparse.SparseTensor(sparse_placeholder[2][0],sparse_placeholder[2][1],sparse_placeholder[2][2])\n",
    "            model.sparsed_weights4=tf.sparse.SparseTensor(sparse_placeholder[3][0],sparse_placeholder[3][1],sparse_placeholder[3][2])\n",
    "            model.last_layer=sparse_placeholder[4].T\n",
    "        sess.run(model.iterator.make_initializer(model.eval_ds))\n",
    "        while True:\n",
    "            try:\n",
    "                options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                t1=time.clock()\n",
    "                pr=sess.run(model.sparse_preds,options=options,run_metadata=run_metadata)\n",
    "                t1=time.clock()-t1\n",
    "                test_total+=t1\n",
    "                fetched_timeline = timeline.Timeline(run_metadata.step_stats)\n",
    "                chrome_trace = fetched_timeline.generate_chrome_trace_format()\n",
    "                with open('timeline_01.json', 'w') as f:\n",
    "                    f.write(chrome_trace)\n",
    "            except:\n",
    "                break\n",
    "        \n",
    "        print(test_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.052399000000019\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        #model.saver.restore(sess,'model/notpruned/model.pb')\n",
    "        test_correct=0\n",
    "        test_total=0\n",
    "        feed_dict={}\n",
    "        sparse_placeholder=sp\n",
    "        model.w_layer1.assign(tf.sparse.to_dense(tf.sparse.SparseTensor(sparse_placeholder[0][0],sparse_placeholder[0][1],sparse_placeholder[0][2])))\n",
    "        model.w_layer2.assign(tf.sparse.to_dense(tf.sparse.SparseTensor(sparse_placeholder[1][0],sparse_placeholder[1][1],sparse_placeholder[1][2])))\n",
    "        model.w_layer3.assign(tf.sparse.to_dense(tf.sparse.SparseTensor(sparse_placeholder[2][0],sparse_placeholder[2][1],sparse_placeholder[2][2])))\n",
    "        model.w_layer4.assign(tf.sparse.to_dense(tf.sparse.SparseTensor(sparse_placeholder[3][0],sparse_placeholder[3][1],sparse_placeholder[3][2])))\n",
    "        sess.run(model.iterator.make_initializer(model.eval_ds))\n",
    "        feed_dict={}\n",
    "        while True:\n",
    "            try:\n",
    "                options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                t=time.clock()\n",
    "                pr=sess.run(model.probs,feed_dict=feed_dict,options=options,run_metadata=run_metadata)\n",
    "                t=time.clock()-t\n",
    "                test_total+=t\n",
    "                fetched_timeline = timeline.Timeline(run_metadata.step_stats)\n",
    "                chrome_trace = fetched_timeline.generate_chrome_trace_format()\n",
    "                with open('timeline_02.json', 'w') as f:\n",
    "                    f.write(chrome_trace)\n",
    "            except:\n",
    "                break\n",
    "        \n",
    "        print(test_total)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/device:CPU:0', '/device:XLA_GPU:0', '/device:XLA_CPU:0']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
